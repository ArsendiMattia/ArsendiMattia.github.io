---
title: Bayesian Copula Modelling of football teams' shots
date: 2024-09-14
comments: true
toc: true
math: true
---

This case study uses a Bayesian Copula approach to model the football teams' shots. 

### Plan

- [**WORK IN PROGRESS**] Introduction. 
- [**WORK IN PROGRESS**] Data Exploration.
- [**WORK IN PROGRESS**] Model Development. 
- [**WORK IN PROGRESS**] Inferential Comparison.

##  Introduction
The following is a snapshot of the dataset used in this case study,

| Team           | Opponent       | Shots          | Venue          | Date           | 
|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|
| arsenal        | forest         | 15             | 1              | 2023-08-12     |
| forest         | arsenal        | 6              | 0              | 2023-08-12     |
| ...            | ...            | ...            | ...            | ...            |
| berlin         | leverkusen     | 2              | 0              | 2023-11-12     | 
| berlin         | augsburg       | 14             | 1              | 2023-11-25     |

More specifically, we have a collection of historical matches of football teams from the top leagues in ENG, ITA, FRA, ESP, and GER for the season 2023/2024, where

*Team*
: refers to the team to which the focus is funneled, 

*Opponent*
: represents the opponent team faced by the considered *Team*, 

*Shots*
: quantifies the number of shots made by the *Team*,

*Venue*
: if 1, the match has been played in the *Team*'s home; otherwise the *Team* is playing away,

*Date*
: is the calendar time in which the match has been played. 

As we can spot, directly modeling the shots made by each team implies assuming that the observations are independent. In other words, the shots made by Arsenal against Nottingham Forest in the game played on 2023-08-12 are independent of those made by Nottingham Forest in the same match and of the shots made by, for example, Union Berlin on 2023-11-25. If the latter case is reasonable since they refer to two different matches, assuming that the shots made by teams in the same game are independent might be an inappropriate assumption. 

Conversely, we can expect that shots made by one team have a substantial impact on the number of shots made by the opponent for different reasons, which might span from more apparent explanations, such as if one team collects many shots, the opponent has less time to organize its offensive maneuver, to more complex ones such as teams which control the game concede less space to the opponents. \\

Therefore, Copula models grant us a flexible approach to model the joint distribution between whichever quantitative variables we are interested in, which are two discrete variables in this specific case study. Specifically, the key feature of a Copula approach is that the marginal distribution of each variable is built separately; only then is the Copula model introduced on top of the marginals to model the dependency between the variables.



% <!-- The main motivation in using a Copula model for modelling football teams' shots is the following: we have 

% that in each match there are two teams facing each other. Therefore a Copula model gives us the flexibility to 

% Let's imagine the scenario in which we have two random varialbes $$Y_{home}$$ and $$Y_{away}$$ which respectivly represent the number of shots executed by specific football teams  -->



<!-- This tutorial aims to share a probabilistic approach for quantifying the uncertainty around the number of shots football teams make, given the matches' characteristics. 

## Dataset
Let's dive directly into a snapshot of the analyzed dataset

| Team           | Opponent       | Shots          | Att_rating     | Def_rating     | Venue          | Time |
|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|:--------------:|:----:|
| arsenal        | forest         | 15             | 82.56          | 73.33          | 1              | 1    |
| arsenal        | crystal        | 13             | 83.00          | 77.00          | 0              | 2    | 
| ...            | ...            | ...            | ...            | ...            | ...            | ...  | 
| berlin         | leverkusen     | 2              | 75.14          | 82.00          | 0              | 11   |
| berlin         | augsburg       | 14             | 75.43          | 73.75          | 1              | 12   |

Specifically, we have a collection of historical matches from the 2023/2024 season until 25/11/2023, considering the teams of the TOP 5 leagues in Europe (ENG, ITA, FRA, ESP, GER). For each match, the following features are gathered:

*Team*
: The team to which the focus is funneled, 

*Opponent*
: The club faced by the considered *Team*, 

*Shots*
: The number of shots made by the *Team*,

*Att_rating*
: Sum of the FIFA ratings (overall) of the considered *Team*'s players in the attack positions, based on the official lineup,

*Def_rating*
: Sum of the FIFA ratings (overall) of the  *Opponent*'s players in the defending positions, based on the official lineup,

*Venue*
: If 1, the match has been played in the *Team*'s home; otherwise the *Team* is playing away,

*Time*
: An index based on the chronological order with which the matches have been played with respect to a specific *Team*. 

> The dataset has been formed by combining [`FbRef`](https://fbref.com/en/) data (using the astonishing library [`worldfootballR`](https://jaseziv.github.io/worldfootballR/)) with the [`EA Sports FC`](https://www.kaggle.com/datasets/stefanoleone992/ea-sports-fc-24-complete-player-dataset?select=male_players.csv) dataset. 
{: .prompt-info }

At this stage, we do know very little about the dataset. However, the following hypotheses are legitim given the general domain of knowledge:

- Each team has its own mean and variance with respect to the number of shots, which can be drastically different among teams.
- We expect to observe more shots when the match is played in the *Team*'s home. 
- Higher is the difference between *Att_rating* and *Def_rating*, higher is the number of shots observed. 
- Matches that have been played closely with respect to the *Time* might be more correlated.

The entire analyzed dataset used is shared [HERE](https://github.com/ArsendiMattia/ArsendiMattia.github.io/blob/main/file-to-share/dataset-shots.csv).

## Data Viz

Let's find out if the data supports the formulated hypotheses.

### Mean(shots) distribution
We can see how the mean shots vary among the teams. Moreover, by segmenting by *Venue*, it can be noticed that the mean shots are higher when the match has been played in the *Team*'s home, as expected.
```R
df %>%
  group_by(Team, Venue) %>%
  summarise(
    shots_mean = mean(Shots)
  ) %>%
  ggplot(aes(x = shots_mean, fill = Venue)) +
  geom_density(alpha = 0.4, linewidth = 2) +
  theme_classic() +
  theme(text = element_text(size = 20), legend.key.size = unit(2, 'cm')) +
  scale_y_continuous(breaks = c(0, .1, .18)) +
  labs(x = "Mean(shots)")
```
![Desktop View](/posts/shots/first.png){: width="972" height="589"}


### Sd(shots) distribution
We can see how the *standard-deviation* of the shots varies among the teams. Moreover, the difference based on the *Venue* is lighter compared to what has been observed in the mean shots distribution. 
```R
df %>%
  group_by(Team, Venue) %>%
  summarise(
    shots_sd = sd(Shots)
  ) %>%
  ggplot(aes(x = shots_sd, fill = Venue)) +
  geom_density(alpha = 0.4, linewidth = 2) +
  theme_classic() +
  theme(text = element_text(size = 20), legend.key.size = unit(2, 'cm')) +
  scale_y_continuous(breaks = c(0, .15, .25)) +
  labs(x = "Sd(shots)")
```
![Desktop View](/posts/shots/second.png){: width="972" height="589"}

### Mean(shots) vs Mean(Att_rating - Def_rating)
A positive linear relationship with the mean shots is observable by taking the mean of the difference between *Att_rating* and *Def_rating* with respect to each team. Moreover, the intercept of the line among the matches played in the *Team*'s home is higher compared to the matches played away.
```r
df %>%
  group_by(Team, Venue) %>%
  summarise(
    shots_mean = mean(Shots),
    Att_Def_diff_mean = mean(Att_rating - Def_rating)
  ) %>%
  ggplot(aes(x = Att_Def_diff_mean, y = shots_mean, color = Venue)) +
  geom_point(size = 10) +
  theme_classic() +
  theme(text = element_text(size = 20), legend.key.size = unit(2, 'cm')) +
  labs(x = "Mean(Att_rating - Def_rating)", y = "Mean(shots)")
```
![Desktop View](/posts/shots/third.png){: width="972" height="589"}

## Complete Bayesian Models
Since the number of shots is positive, a common practice is to model the logarithm scale-free value. Therefore, the following transformation is applied

$$ y = \log \left(\frac{\text{Shots}}{MAD(\text{Shots})}\right),$$

where the *MAD* represents the median absolute deviation function, which is a robust estimation of the standard deviation.

### 1: Linear Regression
The following is the complete Bayesian model adopted in this first iteration

$$ y_n \sim \text{Normal}\left(f(x_n), \sigma\right),$$

where 

$$ f(x_n) = \alpha + \beta_1 \cdot \text{Att_Def_diff}_n + \beta_2 \cdot \text{Venue}_n,$$

$$ \alpha \sim \text{Normal}(0, .2),$$

$$ \beta_1 \sim \text{Normal}(.1, .5),$$

$$ \beta_2 \sim \text{Normal}(.1, .5),$$

$$ \log(\sigma) \sim \text{Normal}(0, .5).$$

The strategy adopted for selecting reasonable prior distributions has been:
1. retrieving the shots of the season 2022/2023,
2. computing the minimum, the maximum, and the quantile values 0.25, 0.5, and 0.75,
3. generating prior predictive distributions with most of the concentration in a specific acceptable interval, namely above the historical minimum, below the historical maximum, and with approximately the exact historical quantiles.

This allows us to reach a reasonable compromise by avoiding using flat priors and obtaining a faster fitting with posterior distributions that are more informative and, hopefully, without divergences. The following is the Stan code for the Prior Predictive Check performed
```stan
data {
  int<lower=1> N;
  vector[N] Att_Def_diff;
  vector[N] Venue;
}
transformed data {
  // Normalization
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;
}
generated quantities {
  real alpha = normal_rng(0, .2);
  real beta1 = normal_rng(.1, .5);
  real beta2 = normal_rng(.1, .5);
  real logsigma = normal_rng(0, .5);

  array[N] real y_gen;
  for(n in 1:N){
    y_gen[n] = normal_rng(
      alpha + beta1 * x1n[n] + beta2 * Venue[n],
      exp(logsigma)
    );
  }
}
```

The following is the Stan code used for drawing representative samples from the posterior distributions
```stan
data {
  int<lower=1> N;
  vector[N] Att_Def_diff;
  vector[N] Venue;
  vector[N] y;
}
transformed data {
  // Normalization
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  real ymean = mean(y);
  real ysd = sd(y);
  vector[N] yn = (y - ymean) / ysd;
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;
}
parameters {  
  real alpha;
  real beta1;
  real beta2;
  real logsigma;
}
model {
  alpha ~ normal(0, .2);
  beta1 ~ normal(.1, .5);
  beta2 ~ normal(.1, .5);
  logsigma ~ normal(0, .5);

  yn ~ normal(
    alpha + beta1 * x1n + beta2 * Venue,
    exp(logsigma)
  );
}
generated quantities {
  vector[N] log_lik;
  for (n in 1:N) log_lik[n] = normal_lpdf(
    yn[n] | alpha + beta1 * x1n[n] + beta2 * Venue[n], exp(logsigma)  
  );
}
```
The behaviors of the chains during the sampling satisfy all the standard requirements. Specifically, each parameter has a Split-$$\hat{R}$$ equal almost to 1 and an *ESS* larger than five times twice the number of the chains. Moreover, there are no divergent transitions.

### 2: Hierarchical Model (Intercept)
Given the assumption that the number of shots tends to look similar among matches belonging to the same team, the complete Bayesian model assumed in this second iteration is the following

$$ y_{nt} \sim \text{Normal}\left(f(x_{nt}), \sigma\right),$$

where 

$$ f(x_{nt}) = \alpha_t + \beta_1 \cdot \text{Att_Def_diff}_{nt} + \beta_2 \cdot \text{Venue}_{nt},$$

$$ \alpha_1, ..., \alpha_t, ... \alpha_T \sim \text{Normal}(\phi, \tau),$$

$$ \phi \sim \text{Normal}(0, .2),$$

$$ \tau \sim \text{HalfStudentT}(3, 0, .5),$$

$$ \beta_1 \sim \text{Normal}(.1, .5),$$

$$ \beta_2 \sim \text{Normal}(.1, .5),$$

$$ \log(\sigma) \sim \text{Normal}(0, .5).$$

$t$ represents a specific *team* among a total of $T$ teams. In other words, we assume that each team has its own intercept value $\alpha_t$ drawn from a common population distribution. 

The strategy adopted for selecting reasonable prior distributions is the same as the one adopted in the first iteration, with the only difference being the following Stan code
```stan
data {
  int<lower=1> N;
  int<lower=1> N_teams;
  array[N] int teams_idx;
  vector[N] Att_Def_diff;
  vector[N] Venue;
}
transformed data {
  // Normalization
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;
}
generated quantities {
  real phi = normal_rng(0, .2);
  real<lower=0> tau = abs(student_t_rng(3, 0, .5));
  vector[N_teams] alpha_tilde = to_vector(normal_rng(
    rep_vector(0, N_teams), 
    rep_vector(1, N_teams)
  ));
  vector[N_teams] alpha = phi + tau * alpha_tilde;

  real beta1 = normal_rng(.1, .5);
  real beta2 = normal_rng(.1, .5);
  real logsigma = normal_rng(0, .5);

  array[N] real y_gen;
  for(n in 1:N){
    y_gen[n] = normal_rng(
      alpha[teams_idx[n]] + beta1 * x1n[n] + beta2 * Venue[n],
      exp(logsigma)
    );
  }
}
```

Representative samples from the posterior distributions are drawn using the Stan code below
```stan
data {
  int<lower=1> N;
  int<lower=1> N_teams;
  array[N] int teams_idx;
  vector[N] Att_Def_diff;
  vector[N] Venue;
  vector[N] y;
}
transformed data {
  // Normalization
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  real ymean = mean(y);
  real ysd = sd(y);
  vector[N] yn = (y - ymean) / ysd;
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;
}
parameters {  
  vector[N_teams] alpha_tilde;
  real phi;
  real<lower=0> tau;
  real beta1;
  real beta2;
  real logsigma;
}
transformed parameters {
  vector[N_teams] alpha;
  for (t in 1:N_teams) {
    alpha[t] = phi + tau * alpha_tilde[t];
  }
   
}
model {
  alpha_tilde ~ std_normal();
  phi ~ normal(0, .2);
  tau ~ student_t(3, 0, 0.5);
  beta1 ~ normal(.1, .5);
  beta2 ~ normal(.1, .5);
  logsigma ~ normal(0, .5);

  yn ~ normal(
    alpha[teams_idx] + beta1 * x1n + beta2 * Venue,
    exp(logsigma)
  );
}
generated quantities {
  vector[N] log_lik;
  for (n in 1:N) log_lik[n] = normal_lpdf(
    yn[n] | alpha[teams_idx[n]] + beta1 * x1n[n] + beta2 * Venue[n], exp(logsigma)  
  );
}
```

The behaviors of the chains satisfied the standard requirements without showing divergences. 

> The `non-centered parametrization` has been used for modeling $\alpha$.
{: .prompt-info }

### 3: Hierarchical Model (Variance)
The complete Bayesian model assumed in this third iteration is described as

$$ y_{nt} \sim \text{Normal}\left(f(x_{n}), \sigma_{t} \right),$$

where 

$$ f(x_{n}) = \alpha + \beta_1 \cdot \text{Att_Def_diff}_{n} + \beta_2 \cdot \text{Venue}_{n},$$

$$ \alpha \sim \text{Normal}(0, .2),$$

$$ \beta_1 \sim \text{Normal}(.1, .5),$$

$$ \beta_2 \sim \text{Normal}(.1, .5),$$

$$ \log(\sigma_1), ..., \log(\sigma_t), ... \log(\sigma_T) \sim \text{Normal}(\omega, \zeta),$$

$$ \omega \sim \text{Normal}(0, .5),$$

$$ \zeta \sim \text{HalfStudentT}(3, 0, .1).$$

In other words, we assume that each team $t$ has its own standard deviation value $\sigma_t$ drawn from a common population distribution. The prior predictive check has been performed using the following Stan code with the same strategy mentioned above 
```stan
data {
  int<lower=1> N;
  int<lower=1> N_teams;
  array[N] int teams_idx;
  vector[N] Att_Def_diff;
  vector[N] Venue;
}
transformed data {
  // Normalization
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;
}
generated quantities {

  real alpha = normal_rng(0, .2);
  real beta1 = normal_rng(.1, .5);
  real beta2 = normal_rng(.1, .5);
  
  real omega = normal_rng(0, .5);
  real<lower=0> zeta = abs(student_t_rng(3, 0, .1));
  vector[N_teams] logsigma_tilde = to_vector(normal_rng(
    rep_vector(0, N_teams), 
    rep_vector(1, N_teams)
  ));
  vector[N_teams] logsigma = omega + zeta * logsigma_tilde;


  array[N] real y_gen;
  for(n in 1:N){
    y_gen[n] = normal_rng(
      alpha + beta1 * x1n[n] + beta2 * Venue[n],
      exp(logsigma[teams_idx[n]])
    );
  }
}
```

The sampling has been performed by running
```stan
data {
  int<lower=1> N;
  int<lower=1> N_teams;
  array[N] int teams_idx;
  vector[N] Att_Def_diff;
  vector[N] Venue;
  vector[N] y;
}
transformed data {
  // Normalization
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  real ymean = mean(y);
  real ysd = sd(y);
  vector[N] yn = (y - ymean) / ysd;
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;
}
parameters {  
  real alpha;
  real beta1;
  real beta2;
  vector[N_teams] logsigma_tilde;
  real omega;
  real<lower=0> zeta;
}
transformed parameters {
  vector[N_teams] logsigma;
  for (t in 1:N_teams) {
    logsigma[t] = omega + zeta * logsigma_tilde[t];
  }
   
}
model {
  alpha ~ normal(0, .2);
  beta1 ~ normal(.1, .5);
  beta2 ~ normal(.1, .5);
  logsigma_tilde ~ std_normal();
  omega ~ normal(0, .5);
  zeta ~ student_t(3, 0, 0.1);

  yn ~ normal(
    alpha + beta1 * x1n + beta2 * Venue,
    exp(logsigma[teams_idx])
  );
}
generated quantities {
  vector[N] log_lik;
  for (n in 1:N) log_lik[n] = normal_lpdf(
    yn[n] | alpha + beta1 * x1n[n] + beta2 * Venue[n], exp(logsigma[teams_idx[n]])  
  );
}
```

The behaviors of the chains satisfied the standard requirements without showing divergences. 

> The `non-centered parametrization` has been used for modeling $\log(\sigma)$.
{: .prompt-info }

### 4: Hierarchical Gaussian Process
In this fourth iteration, a more complex solution is proposed: to model the possible correlation among the matches played closed together with respect to the *Time* variable, a Gaussian Process (GP) is introduced. \\
Specifically, GP will be used to model the residual behavior [^1] of the complete Bayesian model defined in the third iteration by assuming an *Exponentiated quadratic kernel* (RBF) for the *Time* variable. By doing so, modeling the *status* of a team, which might vary throughout the season, is possible. Moreover, the magnitude *s* and length scale *l* parameters of the RBF will be assumed to be different for each team[^2]. It follows that the complete Bayesian model assumed for this iteration is defined as

$$ y_{nt} \sim \text{Normal}\left(f(x_{nt}), \sigma_{t} \right),$$

where 

$$ f(x_{nt}) = \alpha + \beta_1 \cdot \text{Att_Def_diff}_{n} + \beta_2 \cdot \text{Venue}_{n} + f_{nt},$$

$$ \alpha \sim \text{Normal}(0, .2),$$

$$ \beta_1 \sim \text{Normal}(.1, .5),$$

$$ \beta_2 \sim \text{Normal}(.1, .5),$$

$$ f_{1}, ..., f_{t}, ..., f_{T} \sim \text{GP}(0, K_{s_t,l_t}),$$

$$ \log(s_1), ..., \log(s_t), ..., \log(s_T)  \sim \text{Normal}(\exp(s_m), s_s),$$

$$ \log(l_1), ..., \log(l_t), ..., \log(l_T) \sim \text{Normal}(\exp(l_m), l_s),$$

$$ l_m \sim \text{InvGamma}(1, .5),$$

$$ l_s \sim \text{Normal}(0, .2),$$

$$ s_m \sim \text{Normal}(0, .5),$$

$$ s_s \sim \text{Normal}(0, .2),$$
  
$$ \log(\sigma_1), ..., \log(\sigma_t), ... \log(\sigma_T) \sim \text{Normal}(\omega, \zeta),$$

$$ \omega \sim \text{Normal}(0, .5),$$

$$ \zeta \sim \text{HalfStudentT}(3, 0, .1).$$

The Prior Predictive Check has been performed using.
``` stan
data {
  int<lower=1> N;
  int<lower=1> N_teams;
  array[N] int teams_idx;
  vector[N] Att_Def_diff;
  vector[N] Venue;
  
  int<lower=1> N_match_max;
  array[N] int Time;
}
transformed data {
  vector[N_match_max] time;
  for (i in 1:N_match_max) {
    time[i] = i;
  }

  // Normalization
  real tmean = mean(time);
  real tsd = sd(time);
  array[N_match_max] real tn = to_array_1d((time - tmean)/tsd);
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;

  // Numerical stability
  real sigma_intercept = 0.1;
  vector[N_match_max] jitter = rep_vector(1e-9, N_match_max);
}
generated quantities {
  real<lower=0> l_m = abs(inv_gamma_rng(1, .5));
  real<lower=0> l_s = abs(normal_rng(0, .2));
  real<lower=0> s_m = abs(normal_rng(0, .5));
  real<lower=0> s_s = abs(normal_rng(0, .2));

  vector[N_teams] l_tilde = to_vector(normal_rng(
    rep_vector(0, N_teams), 
    rep_vector(1, N_teams)
  ));
  vector[N_teams] s_tilde = to_vector(normal_rng(
    rep_vector(0, N_teams), 
    rep_vector(1, N_teams)
  ));

  vector<lower=0>[N_teams] s; 
  vector<lower=0>[N_teams] l; 
  array[N_teams] matrix[N_match_max, N_match_max] K;
  array[N_teams] matrix[N_match_max, N_match_max] L_K;
  matrix[N_teams, N_match_max] f;
  for (t in 1:N_teams) {
    l[t] = exp(log(l_m) + l_s * l_tilde[t]);
    s[t] = exp(log(s_m) + s_s * s_tilde[t]);
  
    K[t] =  gp_exp_quad_cov(tn, s[t], l[t]) + sigma_intercept^2;
    L_K[t] = cholesky_decompose(add_diag(K[t], jitter));

    vector[N_match_max] eta = to_vector(normal_rng(
      rep_vector(0, N_match_max), 
      rep_vector(1, N_match_max)
    ));

    f[t] = to_row_vector(L_K[t] * eta);
  }

  real omega = normal_rng(0, .5);
  real<lower=0> zeta = abs(student_t_rng(3, 0, 0.1));
  vector[N_teams] tau_tilde = to_vector(normal_rng(
    rep_vector(0, N_teams), 
    rep_vector(1, N_teams)
  ));
  vector[N_teams] tau = omega + zeta * tau_tilde;

  
  real alpha = normal_rng(0, .2);
  real beta1 = normal_rng(.1, .5);
  real beta2 = normal_rng(.1, .5); 

  array[N] real y_gen;
  for(n in 1:N){
    y_gen[n] = normal_rng(
      alpha + beta1 * x1n[n] + beta2 * Venue[n] + f[teams_idx[n], Time[n]],
      exp(tau[teams_idx[n]])
    );
  }
}
```

The Stan code to sample from the posterior distributions is
``` stan
data {
  int<lower=1> N;
  int<lower=1> N_teams;
  array[N] int teams_idx;
  vector[N] Att_Def_diff;
  vector[N] Venue;
  vector[N] y;
  
  int<lower=1> N_match_max;
  array[N] int Time;
}
transformed data {
  vector[N_match_max] time;
  for (i in 1:N_match_max) {
    time[i] = i;
  }

  // Normalization
  real tmean = mean(time);
  real tsd = sd(time);
  array[N_match_max] real tn = to_array_1d((time - tmean)/tsd);
  real x1mean = mean(Att_Def_diff);
  real x1sd = sd(Att_Def_diff);
  real ymean = mean(y);
  real ysd = sd(y);
  vector[N] yn = (y - ymean) / ysd;
  vector[N] x1n = (Att_Def_diff - x1mean) / x1sd;

  // Numerical stability
  real sigma_intercept = 0.1;
  vector[N_match_max] jitter = rep_vector(1e-9, N_match_max);
}
parameters {
  // Population-level parameters
  real<lower=0> l_m;   
  real<lower=0> l_s;   
  real<lower=0> s_m; 
  real<lower=0> s_s; 
  
  // Per-team parameters (non-centered parameterization)
  vector[N_teams] l_tilde;   
  vector[N_teams] s_tilde; 
  
  // GP non-centered parametrization
  vector[N_teams * N_match_max] eta;
  
  real alpha;
  real beta1;
  real beta2;
  real omega;
  real<lower=0> zeta;
  vector[N_teams] logsigma_tilde;
}
transformed parameters {
  // Per-team parameters
  vector<lower=0>[N_teams] s; //signal standard deviation
  vector<lower=0>[N_teams] l; //length scale
  row_vector[N_teams] logsigma;
  
  // Per-team Covariance matrix 
  array[N_teams] matrix[N_match_max, N_match_max] K;
  array[N_teams] matrix[N_match_max, N_match_max] L_K;
  matrix[N_teams, N_match_max] f;
  matrix[N_teams, N_match_max] eta_matrix = to_matrix(eta, N_teams, N_match_max);
  for (t in 1:N_teams) {
    // Non-centered parameterization of per-team parameters
    l[t] = exp(log(l_m) + l_s * l_tilde[t]);
    s[t] = exp(log(s_m) + s_s * s_tilde[t]);
    logsigma[t] = omega + zeta * logsigma_tilde[t];

    K[t] = gp_exp_quad_cov(tn, s[t], l[t]) + sigma_intercept^2;
    L_K[t] = cholesky_decompose(add_diag(K[t], jitter));

    // Non-centered parametrization of per-team GP 
    f[t] = to_row_vector(
      L_K[t] * to_vector(eta_matrix[t])
    ); 
  }
}
model {
  // Priors (on population-level params)
  l_m ~ inv_gamma(1, .5);
  l_s ~ normal(0, .2);
  s_m ~ normal(0, .5);
  s_s ~ normal(0, .2);

  l_tilde ~ std_normal(); //log(l) ~ normal(exp(l_m), l_s)
  s_tilde ~ std_normal();  //log(s) ~ normal(exp(s_m), s_s)
  logsigma_tilde ~ std_normal(); //logsigma ~ normal(omega, zeta);
  
  // Non-centered parametrization GP
  eta ~ std_normal();

  omega ~ normal(0, .5);
  zeta ~ student_t(3, 0, 0.1);
  alpha ~ normal(0, .2);
  beta1 ~ normal(.1, .5);
  beta2 ~ normal(.1, .5);
  
  for (n in 1:N) {
    yn[n] ~ normal(
      alpha + beta1 * x1n[n] + beta2 * Venue[n] + f[teams_idx[n], Time[n]], 
      exp(logsigma[teams_idx[n]]) 
    );
  }
}
generated quantities {
  vector[N] log_lik;
  for (n in 1:N) log_lik[n] = normal_lpdf(
    yn[n] |alpha + beta1 * x1n[n] + beta2 * Venue[n] + f[teams_idx[n], Time[n]], exp(logsigma[teams_idx[n]])  
  );
}
```

> The `non-centered parametrization` has been used for modeling $\log(s)$, $\log(l)$ and for sampling from the GP.
{: .prompt-info }

To improve the warmup and avoid the chains get stuck, the sampling for this iteration has been performed by initialing the NUTS algorithms using the output of Pathfinder[^3] using short paths, as suggested by [Aki Vehtari's case study](https://users.aalto.fi/~ave/casestudies/Birthdays/birthdays.html#Model_1:_Slow_trend). By doing so, the behaviors of the chains satisfied the standard requirements without showing divergences.  

## Predictive performances
The comparison among the predictive performances of the complete Bayesian models assumed in the above iterations has been performed using LOO-CV [^4], which gives the following result

| Iteration      | elpd_diff      | se_diff        |
|:--------------:|:--------------:|:--------------:|
| Model 4        | 0.0            | 0.0            |
| Model 3        | -5.2           | 2.9            |
| Model 2        | -8.4           | 6.6            |
| Model 1        | -12.3          | 7.2            |

Following Occam’s Razor principle, there is not enough evidence in support of the more sophisticated models assumed in the 2, 3, and 4 iterations. Indeed, the simple Bayesian Linear Regression assumed in the first iteration should be preferred, given the advantage of being computationally easier to sample from and evaluate. 

## Reverse Footnote

[^1]: Betancourt, Michael (2020). Robust Gaussian Process Modeling. Retrieved from https://github.com/betanalpha/knitr_case_studies/tree/master/gaussian_processes, commit e10083abbcdb65c745f840ab9d2da58229fa9af3. 
[^2]: Hasz, Brendan (2018). Multilevel Gaussian Processes and Hidden Markov Models with Stan. Retrieved from https://brendanhasz.github.io/2018/11/15/hmm-vs-gp-part2.html
[^3]: Zhang, L., Carpenter, B., Gelman, A. and Vehtari, A. (2022) ‘Pathfinder: Parallel quasi-newton variational inference’, Journal of Machine Learning Research, 23(306).
[^4]: Vehtari, A., Gelman, A. and Gabry, J. (2017) ‘Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC’, Statistics and Computing, 27, pp. 1413–1432. -->
