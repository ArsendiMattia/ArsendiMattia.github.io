---
title: Unlocking the Hessian of the log posterior
date: 2024-09-17
comments: true
toc: true
math: true
---

I want to compute $ \nabla^2_{\boldsymbol{\theta}} \log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) $, where $\boldsymbol{\theta} \in \Theta^J$, $\boldsymbol{x} \in X^N$ and $\boldsymbol{y} \in Y^N$. 

$$
\begin{align}
  \log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) 
  &\propto \log p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{x}) \nonumber \\
  &\propto \log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x}) + \log p(\boldsymbol{x}) + \log p(\boldsymbol{\theta}) \nonumber \\
  &\propto \log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x}) + \log p(\boldsymbol{\theta}) \label{eq:uno} \, , 
\end{align}
$$

where
$$
\begin{align}
\log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x})
&= \log \prod_{n=1}^N p(y_n \mid \boldsymbol{\theta}, x_n) \nonumber \\
&= \log \prod_{n=1}^N \text{Bernoulli}(y_n \mid f(x_n; \boldsymbol{\theta})) \nonumber \\ 
&= \sum_{n=1}^N \log f(x_n; \boldsymbol{\theta})^{y_n} + \log (1 - f(x_n; \boldsymbol{\theta}))^{1-y_n} \label{eq:due} \, .
\end{align}
$$

A typical case assumes a Multivariate Normal distribution as the prior distribution over $\boldsymbol{\theta}$, thus leading to

$$
\begin{align}
p(\boldsymbol{\theta}) \sim \mathcal{N}_J(\boldsymbol{0}, \boldsymbol{\Sigma}) \nonumber \, ,
\end{align}
$$

therefore
$$
\begin{align}
\log p(\boldsymbol{\theta}) \propto -\frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}  \boldsymbol{\theta} \label{eq:tre} \, .
\end{align}
$$

Combining Equation \eqref{eq:due} and Equation \eqref{eq:tre} leads to writing the log posterior distribution \eqref{eq:uno} in terms of

$$
\begin{align}
\log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) \propto \left( \sum_{n=1}^N \log f(x_n; \boldsymbol{\theta})^{y_n} + \log (1 - f(x_n; \boldsymbol{\theta}))^{1-y_n} \right) -\frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}  \boldsymbol{\theta} 
\label{eq:quattro} \, .
\end{align}
$$

Writing the Hessian of Equation \eqref{eq:quattro} in a closed-form solution requires the following partial derivatives

$$
\begin{align}
\frac{\partial f(x_n; \boldsymbol{\theta})}{\partial \theta_j} \nonumber
\end{align}
$$

with respect to each $\theta \in \Theta^J $, which is commonly not tenable to compute manually, especially when $f$ is a Neural Network of many layers, such as in this example. \\
Conversely, ***Reverse-mode automatic differentiation*** is the technique used to efficiently compute this type of gradients. For example, let's imagine a function $f$ equal to

$$
\begin{align}
% f(x_1, x_2) = 1 -  \sigma\left[ 
%   - \log( 1 + \exp(w_1 \cdot x_1 + w_1 \cdot x_2 + b_1)) 
%   - \log( 1 + \exp(w_2 \cdot x_1 + w_2 \cdot x_2 + b_2))
%   - \log( 1 + \exp(w_3 \cdot x_1 + w_3 \cdot x_2 + b_3))
% \right]   
 z_1 = w_{11} \cdot x_1 + w_{12} \cdot x_2 + b_1 \ & z_2 = w_{21} \cdot x_1 + w_{22} \cdot x_2 + b_2 \ & z_3 = w_{31} \cdot x_1 + w_{32} \cdot x_2 + b_3 \ & a_1 = \sigma(z_1) \ & a_2 = \sigma(z_2) \ & a_3 = \sigma(z_3) \ & \text{Output computation:} \ & z_o = v_1 \cdot a_1 + v_2 \cdot a_2 + v_3 \cdot a_3 + b_o \ & f(x_1, x_2) = \sigma(z_o)
\label{eq:cinque} \, , 
\end{align}
$$

where $\sigma$ is a sigmoid function. 


Video [^1]

**WORK IN PROGRESS**

## Reverse Footnote

[^1]: Credits goes to <https://www.youtube.com/watch?v=wG_nF1awSSY>

