---
title: Unlocking the Hessian of the log posterior
date: 2024-09-17
comments: true
toc: true
math: true
---

I want to compute $ \nabla^2_{\boldsymbol{\theta}} \log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) $, where $\boldsymbol{\theta} \in \Theta^J$, $\boldsymbol{x} \in X^N$ and $\boldsymbol{y} \in Y^N$. 

$$
\begin{align}
  \log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) 
  &\propto \log p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{x}) \nonumber \\
  &\propto \log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x}) + \log p(\boldsymbol{x}) + \log p(\boldsymbol{\theta}) \nonumber \\
  &\propto \log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x}) + \log p(\boldsymbol{\theta}) \label{eq:uno} \, , 
\end{align}
$$

where
$$
\begin{align}
\log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x})
&= \log \prod_{n=1}^N p(y_n \mid \boldsymbol{\theta}, x_n) \nonumber \\
&= \log \prod_{n=1}^N \text{Bernoulli}(y_n \mid f(x_n; \boldsymbol{\theta})) \nonumber \\ 
&= \sum_{n=1}^N \log f(x_n; \boldsymbol{\theta})^{y_n} + \log (1 - f(x_n; \boldsymbol{\theta}))^{1-y_n} \label{eq:due} \, .
\end{align}
$$

A typical case assumes a Multivariate Normal distribution as the prior distribution over $\boldsymbol{\theta}$, thus leading to

$$
\begin{align}
p(\boldsymbol{\theta}) \sim \mathcal{N}_J(\boldsymbol{0}, \boldsymbol{\Sigma}) \nonumber \, ,
\end{align}
$$

therefore
$$
\begin{align}
\log p(\boldsymbol{\theta}) \propto -\frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}  \boldsymbol{\theta} \label{eq:tre} \, .
\end{align}
$$

Combining Equation \eqref{eq:due} and Equation \eqref{eq:tre} leads to writing the log posterior distribution \eqref{eq:uno} in terms of

$$
\begin{align}
\log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) \propto \left( \sum_{n=1}^N \log f(x_n; \boldsymbol{\theta})^{y_n} + \log (1 - f(x_n; \boldsymbol{\theta}))^{1-y_n} \right) -\frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}  \boldsymbol{\theta} 
\label{eq:quattro} \, .
\end{align}
$$

Writing the Hessian of Equation \eqref{eq:quattro} in a closed-form solution requires the following partial derivatives

$$
\begin{align}
\frac{\partial f(x_n; \boldsymbol{\theta})}{\partial \theta_j} \nonumber
\end{align}
$$

with respect to each $\theta \in \Theta^J $, which is commonly not tenable to compute manually, especially when $f$ is a Neural Network of many layers, such as in this example. \\
Conversely, ***Reverse-mode automatic differentiation*** is the technique used to efficiently compute this type of gradients. For example, let's imagine a function $f$ equal to

$$
\begin{align}
f(x_1, x_2) = \sigma\left[ \right. \\
  & v_1 \cdot ( 1 + \exp(w_{1,1} \cdot x_1 + w_{1,2} \cdot x_2 + b_1)) + \\
  & v_2 \cdot ( 1 + \exp(w_{2,1} \cdot x_1 + w_{2,2} \cdot x_2 + b_2)) + \\
  & v_3 \cdot ( 1 + \exp(w_{3,1} \cdot x_1 + w_{3,2} \cdot x_2 + b_3)) \\
\left. \right]
\label{eq:cinque} \, ,
\end{align}

$$

where $\sigma$ is a sigmoid function. 


Video [^1]

**WORK IN PROGRESS**

## Reverse Footnote

[^1]: Credits goes to <https://www.youtube.com/watch?v=wG_nF1awSSY>

