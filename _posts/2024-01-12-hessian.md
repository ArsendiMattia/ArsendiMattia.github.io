---
title: Unlocking the Hessian of the log posterior
date: 2024-09-17
comments: true
toc: true
math: true
---

I want to compute $ \nabla^2_{\boldsymbol{\theta}} \log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) $, where $\boldsymbol{\theta} \in \Theta^J$, $\boldsymbol{x} \in X^N$ and $\boldsymbol{y} \in Y^N$. 

$$
\begin{align}
  \log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) 
  &\propto \log p(\boldsymbol{\theta}, \boldsymbol{y}, \boldsymbol{x}) \nonumber \\
  &\propto \log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x}) + \log p(\boldsymbol{x}) + \log p(\boldsymbol{\theta}) \nonumber \\
  &\propto \log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x}) + \log p(\boldsymbol{\theta}) \label{eq:uno} \, , 
\end{align}
$$

where
$$
\begin{align}
\log p(\boldsymbol{y} \mid \boldsymbol{\theta}, \boldsymbol{x})
&= \log \prod_{n=1}^N p(y_n \mid \boldsymbol{\theta}, x_n) \nonumber \\
&= \log \prod_{n=1}^N \text{Bernoulli}(y_n \mid f(x_n; \boldsymbol{\theta})) \nonumber \\ 
&= \sum_{n=1}^N \log f(x_n; \boldsymbol{\theta})^{y_n} + \log (1 - f(x_n; \boldsymbol{\theta}))^{1-y_n} \label{eq:due} \, .
\end{align}
$$

A typical case assumes a Multivariate Normal distribution as the prior distribution over $\boldsymbol{\theta}$, thus leading to

$$
\begin{align}
p(\boldsymbol{\theta}) \sim \mathcal{N}_J(\boldsymbol{0}, \boldsymbol{\Sigma}) \nonumber \, ,
\end{align}
$$

therefore
$$
\begin{align}
\log p(\boldsymbol{\theta}) \propto -\frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}  \boldsymbol{\theta} \label{eq:tre} \, .
\end{align}
$$

Combining Equation \eqref{eq:due} and Equation \eqref{eq:tre} leads to writing the log posterior distribution \eqref{eq:uno} in terms of

$$
\begin{align}
\log p( \boldsymbol{\theta} \mid \boldsymbol{y}, \boldsymbol{x}) \propto \left( \sum_{n=1}^N \log f(x_n; \boldsymbol{\theta})^{y_n} + \log (1 - f(x_n; \boldsymbol{\theta}))^{1-y_n} \right) -\frac{1}{2} \boldsymbol{\theta}^T \boldsymbol{\Sigma}^{-1}  \boldsymbol{\theta} 
\label{eq:quattro} \, .
\end{align}
$$

Writing the Hessian of Equation \eqref{eq:quattro} in a closed-form solution requires the following partial derivatives

$$
\begin{align}
\frac{\partial f(x_n; \boldsymbol{\theta})}{\partial \theta_j} \nonumber
\end{align}
$$

with respect to each $\theta \in \Theta^J $, which is commonly not tenable to compute manually, especially when $f$ is a Neural Network of many layers, such as in this example. \\
Conversely, ***Reverse-mode automatic differentiation***[^1] is the technique used to efficiently compute this type of gradients. For example, let's imagine a function $f$ equal to

$$
\begin{align}
f(x_1, x_2) = \sigma\left[ \right. \nonumber \\
  & z_1 \cdot \sigma(w_{1,1} \cdot x_1 + w_{1,2} \cdot x_2 + b_1) \, + \nonumber \\
  & z_2 \cdot \sigma(w_{2,1} \cdot x_1 + w_{2,2} \cdot x_2 + b_2) \, + \nonumber \\
  & z_3 \cdot \sigma(w_{3,1} \cdot x_1 + w_{3,2} \cdot x_2 + b_3) \nonumber \\
\left. \right]
\label{eq:cinque} \, ,
\end{align}
$$

where $\sigma$ corresponts to a sigmoid function. As we can notice, Function \eqref{eq:cinque} is a standard Neural Network composed of only one single layer of 3 neurons. Moreover, the activation functions are all sigmoid, and the task is a classification since the output is binary. \\
Function \eqref{eq:cinque} can be specified in other terms by writing it as a ***graph*** where the intermediate variables are explicitly specified, as shown in Figure 1.

![Desktop View](/posts/hessian/first.jpg){: width="972" height="589"}
_Figure 1_

Writing the function as a graph helps us to compute the partial derivatives of the output $f$ with respect each of these intermediate variables, such as
$$
\begin{align}
\bar{v}_i = \frac{\partial f}{\partial v_i}
\label{eq:sei} \, ,
\end{align}
$$
which are known with the term ***adjoints***. The beauty of the adjoints \eqref{eq:sei} is that they can be written also as 
$$
\begin{align}
\bar{v}_i = \sum_{j:\text{child of i}} \bar{v}_j \frac{\partial v_j}{\partial v_i} 
\label{eq:sette} \, .
\end{align}



**WORK IN PROGRESS**

## Reverse Footnote

[^1]: Credit goes to <https://www.youtube.com/watch?v=wG_nF1awSSY>

